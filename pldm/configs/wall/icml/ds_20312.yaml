n_steps: &n_steps 16
val_n_steps: *n_steps
env_name: &env_name wall

base_lr: 0.003
data:
  normalize: true
  min_max_normalize_state: true
  dataset_type: DatasetType.Wall
  offline_wall_config:
    n_steps: *n_steps
    use_offline: true
    offline_data_path: "/pldm_envs/wall/presaved_datasets/wall-visual-config_rand_expert_20312-v0.npz"
    lazy_load: false
    batch_size: 64
    device: cuda
    img_size: 65
    train: true
  wall_config:
    action_bias_only: false
    action_noise: 1
    action_angle_noise: 0.2
    action_step_mean: 1.0
    action_step_std: 0.4
    action_lower_bd: 0.2
    action_upper_bd: 1.8
    action_param_xy: true
    batch_size: 64
    device: cuda
    dot_std: 1.3
    border_wall_loc: 5
    fix_wall_batch_k: null
    fix_wall: true
    fix_door_location: 10
    fix_wall_location: 32
    exclude_wall_train: ''
    exclude_door_train: ''
    only_wall_val: ''
    only_door_val: ''
    wall_padding: 20
    door_padding: 10
    wall_width: 3
    door_space: 4
    num_train_layouts: -1
    cross_wall_rate: 0.08
    expert_cross_wall_rate: 0
    img_size: 65
    max_step: 1
    n_steps: *n_steps
    n_steps_reduce_factor: 1
    size: 20000
    val_size: 10000
    train: true
    repeat_actions: 1
# epochs: 3
epochs: 24
eval_at_beginning: false
eval_during_training: false
eval_mpcs: 20
eval_only: false
hjepa:
  train_l1: true
  freeze_l1: false
  disable_l2: true
  l1_n_steps: *n_steps
  level1:
    backbone:
      arch: impala
      backbone_subclass: i
      backbone_mlp: null
      backbone_norm: group_norm #group_norm, batch_norm
      backbone_pool: dim_reduce #dim_reduce, avg_pool
      backbone_final_fc: false
      backbone_width_factor: 2
      channels: 2
      input_dim: null
      final_ln: true
    predictor:
      predictor_arch: rnnV2
      predictor_subclass: '512-512'
      rnn_layers: 1
      z_dim: 0
      z_min_std: 0.1
      residual: true
      predictor_ln: true
      tie_backbone_ln: true
    action_dim: 2
    momentum: 0
  step_skip: 4
load_checkpoint_path: null
load_l1_only: false
objectives_l1:
  objectives:
  - VICReg
  - IDM
  vicreg:
    projector: id
    random_projector: false
    sim_coeff: 1.0
    std_coeff: 2.2
    cov_coeff: 13
    std_coeff_t: 0.19
    cov_coeff_t: 0.0
    sim_coeff_t: 0.5
    cov_per_feature: false
    adjust_cov: true
    cov_chunk_size: null
    std_margin: 1.0
    std_margin_t: 1.0
  idm:
    coeff: 2
    action_dim: 2
    arch: '512'
    arch_subclass: a
    use_pred: false
optimizer_type: Adam
output_dir: test
output_root: /pldm/checkpoints
eval_cfg:
  env_name: *env_name
  log_heatmap: false
  wall_planning:
    n_envs: 100
    seed: 42
    levels: "medium"
    easy:
      n_steps: 50
      n_envs: 100
      max_plan_length: 96
      override_config: true
    medium:
      n_steps: 200
      n_envs: 100
      max_plan_length: 96
      override_config: true
    n_envs_batch_size: 20
    sample_y_min: 32
    sample_y_max: 60
    padding: 1
    n_steps: 200
    level1:  
      planner_type: PlannerType.MPPI
      max_step: 2.45
      min_step: 0
      repr_target: true
      loss_coeff_first: 0.1
      loss_coeff_last: 1
      sum_all_diffs: false
      max_plan_length: 96
      sgd:
        lr: 0.3
        n_iters: 900
        l2_reg: 0
        action_change_reg: 0
        z_reg_coeff: 0
      mppi:
        noise_sigma: 12
        num_samples: 2000
        lambda_: 0.005
        z_reg_coeff: 0
  probing:
    visualize_probing: true
    probe_mpc: false
    probe_encoder: true
    probe_wall: true
    epochs: 20
    epochs_enc: 30
    full_finetune: false
    lr: 0.0002
    probe_targets: "locations"
    locations:
      arch: '512'
    schedule: Constant
    l1_depth: *n_steps
    sample_timesteps: 30
quick_debug: false
run_name: test
run_project: HJEPA-wall
seed: 101
wandb: true
